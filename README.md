# AI vs Human Pull Requests: An Empirical Study on GitHub

This repository contains the data processing scripts and analysis pipeline used in the paper:

**â€œThe Visibilityâ€“Acceptance Paradox: A Large-Scale Study of AI-Generated Pull Requests in Open-Source Softwareâ€**

The project investigates how **AI-generated pull requests (PRs)** compare to **human-generated PRs** in terms of:
- processing speed,
- community feedback,
- and acceptance (merge likelihood).

---

## ğŸ“Œ Research Motivation

Autonomous AI agents are now capable of submitting pull requests without human intervention.  
While these contributions are technically valid and highly active, their integration into open-source communities remains unclear.

This project aims to answer the following question:

> **Do AI-generated pull requests perform as well as human pull requests in real open-source workflows?**

---

## ğŸ”¬ Research Questions

The study is structured around three research questions:

### RQ1 â€“ Speed  
Are AI-generated PRs processed faster than human PRs?
- Time to first comment
- Time to first review
- Total PR duration

### RQ2 â€“ Feedback  
Do AI PRs receive different feedback compared to human PRs?
- Number of comments
- Review approval rate
- Average comment length

### RQ3 â€“ Acceptance  
Are AI PRs more likely to be merged, and in what contexts?
- Merge rate
- Number of commits
- Linkage to GitHub issues

---

## ğŸ“Š Dataset

This project uses the **AIDev dataset**:

> Li et al., *AIDev: A Dataset of AI Agent Contributions to GitHub*, 2025.

The dataset includes:
- pull requests
- users
- comments
- reviews
- commits
- related issues

**Scale of analysis:**  
ğŸ“¦ **932,791 pull requests**

### Author Classification
- PRs are labeled as **AI-generated** if the author login contains `"bot"` (case-insensitive)
- Otherwise, PRs are labeled as **human-generated**

---

## ğŸ§  Key Findings

The analysis reveals a phenomenon called the **Visibilityâ€“Acceptance Paradox**:

- AI PRs receive **faster initial reactions**, often from automated tools.
- They attract **more comments** and are **more frequently linked to issues**.
- However, they:
  - stay open significantly longer,
  - and are **merged less often** than human PRs.

This suggests that AI agents are technically active but struggle with **social integration** in open-source communities.

---

## ğŸ› ï¸ Project Structure

```text
.
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                  # Raw AIDev tables (PRs, comments, reviews, etc.)
â”‚   â””â”€â”€ processed/            # Cleaned and merged datasets
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ build_analysis_table.py   # Builds PR-level analysis table
â”‚   â”œâ”€â”€ compute_metrics.py        # Computes RQ1â€“RQ3 metrics
â”‚
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ figures/              # Plots and visualizations
â”‚   â””â”€â”€ tables/               # Aggregated result tables
â”‚
â”œâ”€â”€ aidev_analysis_ready_table.parquet
â”œâ”€â”€ README.md
â””â”€â”€ paper/
    â””â”€â”€ main.tex               # LaTeX paper
